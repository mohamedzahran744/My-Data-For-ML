{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "Text normalization is the process of transforming text into a canonical (standard) form to reduce variations and inconsistencies. Users express the same concepts in many different ways—for example, love can appear as `lovely`, `luv`, `loveee`, `LOVE`, `loving`, etc. To a machine learning model analyzing sentiment, these variations should ideally be treated as essentially the same word, otherwise the model wastes capacity learning that these are all positive terms.\n",
    "\n",
    "By normalizing text, we reduce vocabulary size, improve model generalization, and help algorithms focus on semantic meaning rather than superficial differences.\n",
    "\n",
    "### Why Text Normalization Matters\n",
    "\n",
    "**Without Normalization:**\n",
    "- `\"I love this!\"` and `\"I LOVE THIS!!\"` treated as completely different\n",
    "- `\"running\"`, `\"runs\"`, `\"ran\"` counted as separate words\n",
    "- Vocabulary explodes: 100,000+ unique tokens\n",
    "- Models struggle to generalize patterns\n",
    "\n",
    "**With Normalization:**\n",
    "- Variations mapped to common forms\n",
    "- Vocabulary reduced: 20,000-30,000 tokens\n",
    "- Better pattern recognition and accuracy\n",
    "- Faster training and inference\n",
    "\n",
    "### Common Normalization Techniques\n",
    "\n",
    "#### 1. **Case Normalization (Lowercasing)**\n",
    "Convert all text to lowercase to eliminate case variations.\n",
    "```python\n",
    "text = \"I LOVE This Product!!!\"\n",
    "normalized = text.lower()\n",
    "# Output: \"i love this product!!!\"\n",
    "```\n",
    "\n",
    "**When to use:** Almost always, unless case carries meaning (e.g., \"US\" vs \"us\")\n",
    "\n",
    "#### 2. **Removing Punctuation & Special Characters**\n",
    "Strip out non-alphanumeric characters that don't contribute to meaning.\n",
    "```python\n",
    "import string\n",
    "text = \"Wow!!! Amazing product... 5/5 stars!!!\"\n",
    "normalized = text.translate(str.maketrans('', '', string.punctuation))\n",
    "# Output: \"Wow Amazing product 55 stars\"\n",
    "```\n",
    "\n",
    "**Trade-off:** May lose information (e.g., `\"don't\"` → `\"dont\"`)\n",
    "\n",
    "#### 3. **Removing Extra Whitespace**\n",
    "Collapse multiple spaces into single spaces.\n",
    "```python\n",
    "import re\n",
    "text = \"Great    product   here\"\n",
    "normalized = re.sub(r'\\s+', ' ', text).strip()\n",
    "# Output: \"Great product here\"\n",
    "```\n",
    "\n",
    "#### 4. **Expanding Contractions**\n",
    "Convert shortened forms to full words.\n",
    "```python\n",
    "contractions = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"it's\": \"it is\"\n",
    "}\n",
    "text = \"I don't think it's bad\"\n",
    "# Output: \"I do not think it is bad\"\n",
    "```\n",
    "\n",
    "#### 5. **Removing Stop Words**\n",
    "Filter out common words that carry little meaning (`the`, `is`, `at`, etc.).\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = \"this is a great product\".split()\n",
    "filtered = [w for w in words if w not in stop_words]\n",
    "# Output: ['great', 'product']\n",
    "```\n",
    "\n",
    "**Caution:** Can remove important context in some tasks (e.g., `\"not good\"` → `\"good\"`)\n",
    "\n",
    "#### 6. **Stemming**\n",
    "Reduce words to their root form by chopping off suffixes (crude but fast).\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"loving\", \"loves\", \"loved\", \"lovely\", \"loveable\"]\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "# Output: ['love', 'love', 'love', 'love', 'love']\n",
    "```\n",
    "\n",
    "**Pros:** Fast, reduces vocabulary significantly  \n",
    "**Cons:** Can create non-words (`\"studies\"` → `\"studi\"`)\n",
    "\n",
    "#### 7. **Lemmatization**\n",
    "Reduce words to their dictionary base form using linguistic rules (more accurate).\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"better\", \"best\"]\n",
    "lemmas = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
    "# Output: ['run', 'run', 'run', 'better', 'best']\n",
    "```\n",
    "\n",
    "**Pros:** Produces real words, linguistically accurate  \n",
    "**Cons:** Slower than stemming, requires POS tagging for best results\n",
    "\n",
    "#### 8. **Handling Repetitions**\n",
    "Normalize elongated words (e.g., `\"yessss\"` → `\"yes\"`).\n",
    "```python\n",
    "import re\n",
    "def normalize_repetition(text):\n",
    "    # Reduce 3+ repeated characters to 2\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "text = \"I loooove this soooo much!!!\"\n",
    "normalized = normalize_repetition(text)\n",
    "# Output: \"I loove this sooo much!!!\"\n",
    "```\n",
    "\n",
    "#### 9. **Number Normalization**\n",
    "Replace numbers with tokens or remove them entirely.\n",
    "```python\n",
    "text = \"I bought 3 items for $25.99\"\n",
    "normalized = re.sub(r'\\d+', '<NUM>', text)\n",
    "# Output: \"I bought <NUM> items for $<NUM>.<NUM>\"\n",
    "```\n",
    "\n",
    "#### 10. **Accent Removal (Diacritics)**\n",
    "Normalize accented characters to ASCII.\n",
    "```python\n",
    "import unicodedata\n",
    "text = \"café naïve résumé\"\n",
    "normalized = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "# Output: \"cafe naive resume\"\n",
    "```\n",
    "\n",
    "### When to Apply Different Techniques\n",
    "\n",
    "| Technique | Sentiment Analysis | Topic Modeling | Search | NER | Translation |\n",
    "|-----------|-------------------|----------------|--------|-----|-------------|\n",
    "| Lowercasing | ✅ Yes | ✅ Yes | ✅ Yes | ❌ No* | ✅ Yes |\n",
    "| Remove Punctuation | ⚠️ Careful** | ✅ Yes | ⚠️ Careful | ❌ No | ✅ Yes |\n",
    "| Stemming | ✅ Yes | ✅ Yes | ✅ Yes | ❌ No | ❌ No |\n",
    "| Lemmatization | ✅ Yes | ✅ Yes | ✅ Yes | ⚠️ Careful | ⚠️ Careful |\n",
    "| Stop Words | ⚠️ Careful*** | ✅ Yes | ✅ Yes | ❌ No | ❌ No |\n",
    "\n",
    "\\* Capitalization matters for named entities  \n",
    "\\*\\* Can lose negation: `\"don't\"` → `\"dont\"`  \n",
    "\\*\\*\\* `\"not good\"` becomes `\"good\"` if \"not\" is removed\n",
    "\n",
    "### Trade-offs to Consider\n",
    "\n",
    "**Over-normalization Risks:**\n",
    "- Loss of sentiment: `\"NOT good\"` → `\"good\"`\n",
    "- Loss of entities: `\"US\"` (country) → `\"us\"` (pronoun)\n",
    "- Loss of context: `\"Apple\"` (company) vs `\"apple\"` (fruit)\n",
    "\n",
    "**Under-normalization Risks:**\n",
    "- Vocabulary explosion\n",
    "- Poor generalization\n",
    "- Increased computational cost\n",
    "\n",
    "**Best Practice:** Start conservative, normalize incrementally, and evaluate impact on your specific task through experimentation.\n",
    "\n",
    "So in this notebook, we will discuss these methods in detail and explore how to implement them effectively for different NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress, fli, die, mule, deni, die, agre, own, humbl, size, meet, state, siez, item, sensat, tradit, refer, colon, plot\n"
     ]
    }
   ],
   "source": [
    "# PortStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer', \n",
    "            'plotted']\n",
    "\n",
    "singles = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(', '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Snowball stemmer support different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic, danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, porter, portuguese, romanian, russian, spanish, swedish\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'جو سماوه صاف'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "ar_stemmer.stem(\"الجو سماؤه صافية\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "corpuses : corpus\n",
      "better : good\n",
      "best : best\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"corpuses :\", lemmatizer.lemmatize(\"corpuses\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"best :\", lemmatizer.lemmatize(\"best\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lemmatization vs Stemming\n",
    "\n",
    "The key concept here is that stemming sometime destroy the word unlike lemmatization where we keep the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## `Great Job`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zahran",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
